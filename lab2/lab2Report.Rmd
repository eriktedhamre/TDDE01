---
title: "lab1"
author: "Erik Tedhamre"
date: "8 December 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 1.1
```{r 1.1}
library("ggplot2")
library("MASS")
data <- data.frame(read.csv("australian-crabs.csv"))
ggplot(data = data, mapping = aes(CL, RW, color = sex)) + geom_point()
```
A plot of carapace length versus rear width where the observations are colored by sex. Looking at the graph the data seems reasonably easy to classify by linear discriminant analysis. Because there seems to be a line between the two sexes.

## Assignment 1.2
```{r 1.2}
lda.model1 <- lda(sex ~ CL + RW, data = data)
lda.predict1 <- predict(lda.model1, data)
ggplot.0.5 <- ggplot(data = data, mapping = aes(CL, RW, color = lda.predict1$class, shape = sex )) + geom_point()
ggplot.0.5
mcr.0.5 <- mean(lda.predict1$class != data$sex)
```

The missclassification rate for the linear discriminant analysis is **0.035**. This is pretty reasonable considering we saw on the original graph that there was one area with a bit of an overlap. If this is good enough for actual use is hard to say, it mostly depends on how much we would lose on an incorrect classification.

## Assignment 1.3
```{r 1.3.1}
lda.model2 <- lda(sex ~ CL + RW, data = data, prior = c(Female = 0.1, Male = 0.9))
lda.predict2 <- predict(lda.model2, data)
ggplot(data = data, mapping = aes(CL, RW, color = lda.predict2$class, shape = sex )) + geom_point()
```

The number of males increased since we are assuming a wheighted distribution. Especially the areas containing both types of observations are now classified as only males instead of both.

```{r 1.3.2}
mcr.0.9 <- mean(lda.predict2$class != data$sex)
```

The missclassification rate for the weighted linear discriminant analysis is **0.08**.

## Assignment 1.4
```{r 1.4.1}
glm.model <- glm(as.factor(sex) ~ CL + RW, family = binomial, data = data)
glm.predict <- predict(glm.model, data, type = 'response')
mcr.glm <- mean(glm.predict != data$sex)
```
The missclassification rate is **0.035** which is the same as the original linear discriminant analysis.

```{r 1.4.2}
glm.predict.0.5 <- ifelse(glm.predict > 0.5, "Male", "Female")
glm.slope <- coef(glm.model)[2]/(-coef(glm.model)[3])
glm.intercept <- coef(glm.model)[1]/(-coef(glm.model)[3])
ggplot.0.5 + geom_abline(slope = glm.slope, intercept = glm.intercept)
```
The decision line is drawn in the graph.

## Assignment 2
Splitting the data into partitions
```{r 2.1.1}
library("e1071")
library("MASS")
library("tree")
library("ggplot2")
setwd("~/TDDE01/lab2")
data.credit <- data.frame(read.csv("creditscoring.csv"))
n=dim(data.credit)[1]
set.seed(12345) 
id=sample(1:n, floor(n*0.5)) 
train=data.credit[id,]
id1=setdiff(1:n, id)
set.seed(12345) 
id2=sample(id1, floor(n*0.25)) 
valid=data.credit[id2,]
id3=setdiff(id1,id2)
test=data.credit[id3,]
```


The models used in calculating the following confusion matrices.
```{r}
tree.fit.dev <- tree(good_bad ~., data = train, split = "deviance")
tree.fit.gini <- tree(good_bad ~., data = train, split = "gini")
```


```{r}
pred.dev.train <- predict(tree.fit.dev, newdata = train, type="class")
mean(train$good_bad != pred.dev.train)
```

Missclassification rate for deviance on train data.

```{r}
pred.dev.test <- predict(tree.fit.dev, newdata = test, type="class")
mean(test$good_bad != pred.dev.test)
```

Missclassification rate for deviance on test data.

```{r}
pred.gini.train <- predict(tree.fit.gini, newdata = train, type="class")
mean(train$good_bad != pred.gini.train)
```

Missclassification rate for gini on train data.

```{r}
pred.gini.test <- predict(tree.fit.gini, newdata = test, type="class")
mean(test$good_bad != pred.gini.test)
```

Missclassification rate for gini on test data.

The confusion matrix is the best for deviance compared to gini based on the number of correct predictions for the test data.

## Assignment 2.3
```{r echo = FALSE}
fit=tree(good_bad~., data=train)
set.seed(12345)
max.size <- 15
trainScore=rep(0,max.size)
testScore=rep(0,max.size)
for(i in 2:max.size) {
  prunedTree=prune.tree(fit,best=i)
  pred=predict(prunedTree, newdata=valid, type="tree")
  trainScore[i]=deviance(prunedTree)
  testScore[i]=deviance(pred)
}
plot(2:max.size, trainScore[2:max.size], type="b", col="red", ylim=c(250,600), xlab = "Number of leaves", ylab = "Score")
points(2:max.size, testScore[2:max.size], type="b", col="blue")
```

Looking at the graph we see a minimum value for 4.

```{r echo = FALSE}
set.seed(12345)
finalTree=prune.tree(fit, best=4)
plot(finalTree)
```

The optimal depth of the tree is three which can be seen in the graph.

```{r echo = FALSE}
set.seed(12345)
Yfit=predict(finalTree, newdata=valid, type = "class")
table(actual = valid$good_bad, predicted = Yfit)
```

Confusion matrix for the validation data for the tree data

```{r echo = FALSE}
summary(finalTree)
```

As seen above the variables used in the tree are "savings", "duration" and "history".

```{r echo = FALSE}
mean(valid$good_bad != Yfit)
```

Missclassification rate is **0.264**.

## Assignment 2.4

```{r echo = FALSE}
fit.bayes=naiveBayes(good_bad~., data=train)
Yfit.bayes.train=predict(fit.bayes, newdata=train)
Yfit.bayes.test=predict(fit.bayes, newdata=test)
table(actual = Yfit.bayes.train, predicted = train$good_bad)
mean(Yfit.bayes.train != train$good_bad)
```

Missclassification rate for bayes on train data.

```{r echo = FALSE}
table(actual = Yfit.bayes.test, predicted = test$good_bad)
mean(Yfit.bayes.test != test$good_bad)
```
Missclassification rate for bayes on test data.

The tree prediction is a bit better than the bayesian prediction.

```{r echo = FALSE}
set.seed(12345)

createROCmatrix <- function(pred, pi.vector){
  tpr.vector = numeric()
  fpr.vector = double()
  
  for (pi.value in pi.vector) {
    predict.pi <- ifelse(pred[,2] > pi.value, "good", "bad")
    TP <- length(which(predict.pi == "good" & test$good_bad == "good"))
    TN <- length(which(predict.pi == "bad" & test$good_bad == "bad"))
    FP <- length(which(predict.pi == "good" & test$good_bad == "bad"))
    FN <- length(which(predict.pi == "bad" & test$good_bad == "good"))
    
    tpr.vector <- append(tpr.vector, TP/(TP+FN))
    fpr.vector <- append(fpr.vector, FP/(FP+TN))
  }
  return(data.frame(pi.vector, tpr.vector, fpr.vector))
}

Yfit.tree.test=predict(finalTree, newdata=test)
pi.vector <- seq(0.05, 0.95, 0.05)

tree.predict <- predict(finalTree, newdata = test)
fit.bayes <- naiveBayes(good_bad~., data=train)
bayes.predict <- predict(fit.bayes, newdata = test, type = "raw")

tree.ROC.matrix <- createROCmatrix(tree.predict, pi.vector)
bayes.ROC.matrix <- createROCmatrix(bayes.predict, pi.vector)

ggplot(data = NULL, aes(col = classifier), xlab = "FPR", ylab = "TPR") +
  geom_point(data = tree.ROC.matrix, aes(x = fpr.vector, y = tpr.vector, col="Tree")) + 
  geom_line(data = tree.ROC.matrix, aes(x = fpr.vector, y = tpr.vector, col="Tree")) +
  geom_point(data = bayes.ROC.matrix, aes(x = fpr.vector, y = tpr.vector, col="Bayes")) + 
  geom_line(data = bayes.ROC.matrix, aes(x = fpr.vector, y = tpr.vector, col="Bayes")) 

```

Graph of radius of convergence


```{r echo = FALSE}
bayes.predict.train <- predict(fit.bayes, newdata = train, type = "raw")
loss_matrix <- matrix(data = c(0,1,10,0), nrow = 2, ncol = 2)
bayes.loss.predict <- ifelse(bayes.predict[,2]/bayes.predict[,1] > loss_matrix[3]/loss_matrix[2], "good", "bad")
bayes.loss.predict.train <- ifelse(bayes.predict.train[,2]/bayes.predict.train[,1] > loss_matrix[3]/loss_matrix[2], "good", "bad")
table(actual = test$good_bad, predict = bayes.loss.predict)
```

Confusion matrix for bayes with loss matrix on test data.

```{r echo = FALSE}
table(actual = train$good_bad, predict = bayes.loss.predict.train)
```

Confusion matrix for bayes with loss matrix on train data.

It's a lot more expensive to have a false positive than it is to have a false negative according to a loss matrix.
Meaning that we will be more cautious with the "good" classification. This means that we should have a much higher frequency of data being classified as "bad".

## Assignment 4.1
```{r echo = FALSE}
library("ggplot2")
library("fastICA")
setwd("~/TDDE01/lab2")
data <- data.frame(read.csv2("NIRSpectra.csv"))
data.features <- subset(data, select = -c(Viscosity))
pca.fit <-prcomp(x = data.features, center = TRUE, scale. = TRUE)
plot(pca.fit, type="l", main = "Principal component variance dependency")
```

By looking at summary(pca.fit), can see that PC1 and PC2 cumulatively explains 99% of the variance. The summary(pca.fit) is not printed here since it's output is very large.

```{r echo = FALSE}
qplot(x=pca.fit$x[,1], y=pca.fit$x[,2], data=pca.fit$scores, xlab = "PC1", ylab = "PC2")
```

Plot of the scores in the in the (PC1, PC2) coordinates. There seems to be two fuels that differ greatly from the other, in that they have a much higher coefficent for PC1, while a lot of the other observations seem to lie close to zero.

## Assignment 4.2

```{r echo = FALSE}
plot(pca.fit$rotation[,1], main = "Traceplot", ylab = "PC1")
plot(pca.fit$rotation[,2], main = "Traceplot", ylab = "PC2")
```

## Assignment 4.3
```{r echo = FALSE}
set.seed(12345)
a <- fastICA(X = data.features, 2, alg.typ = "parallel", fun = "logcosh", alpha = 1, method = "R", row.norm = FALSE, maxit = 200, tol = 0.0001, verbose = TRUE)
W.prim = a$K %*% a$W
plot(W.prim[,1], main = "Traceplot", ylab = "W' column 1")
plot(W.prim[,2], main = "Traceplot", ylab = "W' column 2")
```


```{r echo = FALSE}
qplot(x = a$S[,1], y = a$S[,2], data = data.features, xlab = "Latent feature 1", ylab = "Latent feature 2")
```
